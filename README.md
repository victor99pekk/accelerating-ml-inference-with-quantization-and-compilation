# Accelerating ML Inference with Quantization and Compilation

**Title**: Accelerating Deep Learning Model Inference through Quantization and Compilation: A Comparative Study

**Abstract**:  
This study explores the effectiveness of post-training quantization and compiler-based optimizations in accelerating deep learning model inference. We benchmark a ResNet-18 model under four different configurations: baseline FP32, INT8 quantized, compiled FP32, and compiled INT8. Metrics such as inference latency, memory usage, model size, and accuracy are collected and analyzed. The goal is to understand the trade-offs between quantization and compilation techniques, individually and combined, to optimize model deployment for efficient real-world applications.

---