# Accelerating ML Inference with Quantization and Compilation

This project explores how post-training quantization and compiler-based optimizations can significantly accelerate deep learning model inference. We benchmark the performance of FP32 models versus INT8-optimized models compiled with TVM and TorchScript.
