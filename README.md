# accelerating-ml-inference-with-quantization-and-compilation
Accelerating ML model inference by applying quantization and compiler optimizations. Performance benchmarks for FP32 vs INT8 models
